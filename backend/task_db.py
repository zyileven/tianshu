"""
MinerU Tianshu - SQLite Task Database Manager
å¤©æ¢ä»»åŠ¡æ•°æ®åº“ç®¡ç†å™¨

è´Ÿè´£ä»»åŠ¡çš„æŒä¹…åŒ–å­˜å‚¨ã€çŠ¶æ€ç®¡ç†å’ŒåŸå­æ€§æ“ä½œ

æ¶æ„è¯´æ˜ (Hybrid Queue):
    - SQLite: ä»»åŠ¡å…ƒæ•°æ®å­˜å‚¨ã€å†å²è®°å½•ã€ç»“æœç®¡ç†
    - Redis (å¯é€‰): é«˜æ€§èƒ½ä»»åŠ¡é˜Ÿåˆ—ã€ä¼˜å…ˆçº§è°ƒåº¦
    - å½“ Redis å¯ç”¨æ—¶ï¼Œé˜Ÿåˆ—æ“ä½œç”± Redis å¤„ç†
    - å½“ Redis ä¸å¯ç”¨æ—¶ï¼Œè‡ªåŠ¨å›é€€åˆ° SQLite
"""

import sqlite3
import json
import uuid
from contextlib import contextmanager
from typing import Optional, List, Dict
from pathlib import Path
from loguru import logger

# å¯¼å…¥ Redis é˜Ÿåˆ—ï¼ˆå¯é€‰ï¼‰
try:
    from redis_queue import get_redis_queue
    REDIS_QUEUE_AVAILABLE = True
except ImportError:
    REDIS_QUEUE_AVAILABLE = False
    def get_redis_queue():
        return None


class TaskDB:
    """ä»»åŠ¡æ•°æ®åº“ç®¡ç†ç±»"""

    def __init__(self, db_path=None):
        # å¯¼å…¥æ‰€éœ€æ¨¡å—
        import os
        from pathlib import Path

        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„è·¯å¾„ï¼Œå…¶æ¬¡ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œæœ€åä½¿ç”¨é»˜è®¤è·¯å¾„
        if db_path is None:
            db_path = os.getenv("DATABASE_PATH", "/app/data/db/mineru_tianshu.db")
            # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„
            db_path = str(Path(db_path).resolve())
        else:
            # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„
            db_path = str(Path(db_path).resolve())

        # ç¡®ä¿ db_path æ˜¯ç»å¯¹è·¯å¾„å­—ç¬¦ä¸²
        self.db_path = str(Path(db_path).resolve())
        self._init_db()

    def _get_conn(self):
        """è·å–æ•°æ®åº“è¿æ¥ï¼ˆæ¯æ¬¡åˆ›å»ºæ–°è¿æ¥ï¼Œé¿å… pickle é—®é¢˜ï¼‰

        å¹¶å‘å®‰å…¨è¯´æ˜ï¼š
            - ä½¿ç”¨ check_same_thread=False æ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºï¼š
              1. æ¯æ¬¡è°ƒç”¨éƒ½åˆ›å»ºæ–°è¿æ¥ï¼Œä¸è·¨çº¿ç¨‹å…±äº«
              2. è¿æ¥ä½¿ç”¨å®Œç«‹å³å…³é—­ï¼ˆåœ¨ get_cursor ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­ï¼‰
              3. ä¸ä½¿ç”¨è¿æ¥æ± ï¼Œé¿å…çº¿ç¨‹é—´å…±äº«åŒä¸€è¿æ¥
            - timeout=30.0 é˜²æ­¢æ­»é”ï¼Œå¦‚æœé”ç­‰å¾…è¶…è¿‡30ç§’ä¼šæŠ›å‡ºå¼‚å¸¸
        """
        conn = sqlite3.connect(self.db_path, check_same_thread=False, timeout=30.0)
        conn.row_factory = sqlite3.Row
        return conn

    @contextmanager
    def get_cursor(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨æäº¤å’Œé”™è¯¯å¤„ç†"""
        conn = self._get_conn()
        cursor = conn.cursor()
        try:
            yield cursor
            conn.commit()
        except Exception as e:
            conn.rollback()
            raise e
        finally:
            conn.close()  # å…³é—­è¿æ¥

    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        with self.get_cursor() as cursor:
            # åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS tasks (
                    task_id TEXT PRIMARY KEY,
                    file_name TEXT NOT NULL,
                    file_path TEXT,
                    status TEXT DEFAULT 'pending',
                    priority INTEGER DEFAULT 0,
                    backend TEXT DEFAULT 'pipeline',
                    options TEXT,
                    result_path TEXT,
                    error_message TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    started_at TIMESTAMP,
                    completed_at TIMESTAMP,
                    worker_id TEXT,
                    retry_count INTEGER DEFAULT 0
                )
            """)

            # åˆ›å»ºåŸºç¡€ç´¢å¼•
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_status ON tasks(status)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_priority ON tasks(priority DESC)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_created_at ON tasks(created_at)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_worker_id ON tasks(worker_id)")

            # è¿ç§»ï¼šæ·»åŠ  parent_task_id ç­‰å­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            try:
                cursor.execute("SELECT parent_task_id FROM tasks LIMIT 1")
            except sqlite3.OperationalError:
                # å­—æ®µä¸å­˜åœ¨ï¼Œæ·»åŠ æ–°å­—æ®µ
                logger.info("ğŸ“Š Migrating database schema: adding parent-child task support")
                cursor.execute("ALTER TABLE tasks ADD COLUMN parent_task_id TEXT")
                cursor.execute("ALTER TABLE tasks ADD COLUMN is_parent INTEGER DEFAULT 0")
                cursor.execute("ALTER TABLE tasks ADD COLUMN child_count INTEGER DEFAULT 0")
                cursor.execute("ALTER TABLE tasks ADD COLUMN child_completed INTEGER DEFAULT 0")
                logger.info("âœ… Parent-child task fields added")

            # åˆ›å»ºä¸»å­ä»»åŠ¡ç´¢å¼•ï¼ˆå­—æ®µå­˜åœ¨åæ‰åˆ›å»ºï¼‰
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_parent_task ON tasks(parent_task_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_is_parent ON tasks(is_parent)")

            # è¿ç§»ï¼šæ·»åŠ  user_id å­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            try:
                cursor.execute("SELECT user_id FROM tasks LIMIT 1")
            except sqlite3.OperationalError:
                logger.info("ğŸ“Š Migrating database schema: adding user_id field")
                cursor.execute("ALTER TABLE tasks ADD COLUMN user_id TEXT")
                logger.info("âœ… user_id field added")

    def create_task(
        self,
        file_name: str,
        file_path: str,
        backend: str = "pipeline",
        options: dict = None,
        priority: int = 0,
        user_id: str = None,
    ) -> str:
        """
        åˆ›å»ºæ–°ä»»åŠ¡

        Args:
            file_name: æ–‡ä»¶å
            file_path: æ–‡ä»¶è·¯å¾„
            backend: å¤„ç†åç«¯ (pipeline/vlm-transformers/vlm-vllm-engine)
            options: å¤„ç†é€‰é¡¹ (dict)
            priority: ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå¤§è¶Šä¼˜å…ˆ
            user_id: ç”¨æˆ·ID (å¯é€‰,ç”¨äºæƒé™æ§åˆ¶)

        Returns:
            task_id: ä»»åŠ¡ID
        """
        task_id = str(uuid.uuid4())
        with self.get_cursor() as cursor:
            cursor.execute(
                """
                INSERT INTO tasks (task_id, file_name, file_path, backend, options, priority, user_id)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
                (task_id, file_name, file_path, backend, json.dumps(options or {}), priority, user_id),
            )

        # å…¥é˜Ÿåˆ° Redisï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self._enqueue_to_redis(task_id, priority, {
            "file_name": file_name,
            "backend": backend,
        })

        return task_id

    def _enqueue_to_redis(self, task_id: str, priority: int, task_data: dict = None) -> bool:
        """
        å°†ä»»åŠ¡åŠ å…¥ Redis é˜Ÿåˆ—

        Args:
            task_id: ä»»åŠ¡ID
            priority: ä¼˜å…ˆçº§
            task_data: å¯é€‰çš„ä»»åŠ¡å¿«ç…§æ•°æ®

        Returns:
            bool: æ˜¯å¦æˆåŠŸå…¥é˜Ÿåˆ° Redis
        """
        if not REDIS_QUEUE_AVAILABLE:
            return False

        redis_queue = get_redis_queue()
        if redis_queue:
            try:
                return redis_queue.enqueue(task_id, priority, task_data)
            except Exception as e:
                logger.warning(f"âš ï¸  Failed to enqueue to Redis, SQLite fallback active: {e}")
        return False

    def get_next_task(self, worker_id: str, max_retries: int = 3) -> Optional[Dict]:
        """
        è·å–ä¸‹ä¸€ä¸ªå¾…å¤„ç†ä»»åŠ¡ï¼ˆåŸå­æ“ä½œï¼Œé˜²æ­¢å¹¶å‘å†²çªï¼‰

        Args:
            worker_id: Worker ID
            max_retries: å½“ä»»åŠ¡è¢«å…¶ä»– worker æŠ¢èµ°æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤3æ¬¡ï¼‰

        Returns:
            task: ä»»åŠ¡å­—å…¸ï¼Œå¦‚æœæ²¡æœ‰ä»»åŠ¡è¿”å› None

        å¹¶å‘å®‰å…¨è¯´æ˜ï¼ˆSQLite æ¨¡å¼ï¼‰ï¼š
            1. ä½¿ç”¨ BEGIN IMMEDIATE ç«‹å³è·å–å†™é”
            2. UPDATE æ—¶æ£€æŸ¥ status = 'pending' é˜²æ­¢é‡å¤æ‹‰å–
            3. æ£€æŸ¥ rowcount ç¡®ä¿æ›´æ–°æˆåŠŸ
            4. å¦‚æœä»»åŠ¡è¢«æŠ¢èµ°ï¼Œç«‹å³é‡è¯•è€Œä¸æ˜¯è¿”å› Noneï¼ˆé¿å…ä¸å¿…è¦çš„ç­‰å¾…ï¼‰

        Redis æ¨¡å¼ï¼š
            1. ä½¿ç”¨ BZPOPMIN åŸå­è·å–æœ€é«˜ä¼˜å…ˆçº§ä»»åŠ¡
            2. ä»»åŠ¡è‡ªåŠ¨ç§»å…¥ processing set
            3. ä» SQLite è·å–å®Œæ•´ä»»åŠ¡æ•°æ®
        """
        from loguru import logger

        # å°è¯•ä½¿ç”¨ Redis é˜Ÿåˆ—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        task = self._get_next_task_redis(worker_id)
        if task is not None:
            return task

        # Redis ä¸å¯ç”¨æˆ–å‡ºé”™ï¼Œå›é€€åˆ° SQLite
        for attempt in range(max_retries):
            try:
                with self.get_cursor() as cursor:
                    # ä½¿ç”¨äº‹åŠ¡ç¡®ä¿åŸå­æ€§
                    cursor.execute("BEGIN IMMEDIATE")

                    # æŒ‰ä¼˜å…ˆçº§å’Œåˆ›å»ºæ—¶é—´è·å–ä»»åŠ¡
                    cursor.execute("""
                        SELECT * FROM tasks
                        WHERE status = 'pending'
                        ORDER BY priority DESC, created_at ASC
                        LIMIT 1
                    """)

                    task = cursor.fetchone()
                    if task:
                        task_id = task["task_id"]
                        # ç«‹å³æ ‡è®°ä¸º processingï¼Œå¹¶ç¡®ä¿çŠ¶æ€ä»æ˜¯ pending
                        cursor.execute(
                            """
                            UPDATE tasks
                            SET status = 'processing',
                                started_at = CURRENT_TIMESTAMP,
                                worker_id = ?
                            WHERE task_id = ? AND status = 'pending'
                        """,
                            (worker_id, task_id),
                        )

                        # æ£€æŸ¥æ˜¯å¦æ›´æ–°æˆåŠŸï¼ˆé˜²æ­¢è¢«å…¶ä»– worker æŠ¢èµ°ï¼‰
                        if cursor.rowcount == 0:
                            # ä»»åŠ¡è¢«å…¶ä»–è¿›ç¨‹æŠ¢èµ°äº†ï¼Œç«‹å³é‡è¯•
                            # å› ä¸ºé˜Ÿåˆ—ä¸­å¯èƒ½è¿˜æœ‰å…¶ä»–å¾…å¤„ç†ä»»åŠ¡
                            if attempt == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶è®°å½•æ—¥å¿—
                                logger.debug(f"Task {task_id} was grabbed by another worker, retrying...")
                            continue

                        return dict(task)
                    else:
                        # é˜Ÿåˆ—ä¸­æ²¡æœ‰å¾…å¤„ç†ä»»åŠ¡ï¼Œè¿”å› None
                        # åªåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶è®°å½•è°ƒè¯•ä¿¡æ¯ï¼ˆé¿å…æ—¥å¿—è¿‡å¤šï¼‰
                        if attempt == 0:
                            # æ£€æŸ¥æ˜¯å¦æœ‰ pending ä»»åŠ¡ï¼ˆç”¨äºè¯Šæ–­ï¼‰
                            cursor.execute("SELECT COUNT(*) as count FROM tasks WHERE status = 'pending'")
                            pending_count = cursor.fetchone()["count"]
                            if pending_count > 0:
                                logger.warning(
                                    f"âš ï¸  Found {pending_count} pending tasks but failed to grab one "
                                    f"(attempt {attempt + 1}/{max_retries})"
                                )
                        return None

            except Exception as e:
                logger.error(f"âŒ Error in get_next_task (attempt {attempt + 1}/{max_retries}): {e}")
                logger.exception(e)
                if attempt == max_retries - 1:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè¿”å› None
                    return None
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´åé‡è¯•
                import time

                time.sleep(0.1)

        # é‡è¯•æ¬¡æ•°ç”¨å°½ï¼Œä»æœªè·å–åˆ°ä»»åŠ¡ï¼ˆé«˜å¹¶å‘åœºæ™¯ï¼‰
        logger.warning(f"âš ï¸  Failed to get task after {max_retries} attempts")
        return None

    def _get_next_task_redis(self, worker_id: str) -> Optional[Dict]:
        """
        ä» Redis é˜Ÿåˆ—è·å–ä¸‹ä¸€ä¸ªä»»åŠ¡

        Args:
            worker_id: Worker ID

        Returns:
            task: ä»»åŠ¡å­—å…¸ï¼Œå¦‚æœ Redis ä¸å¯ç”¨æˆ–æ— ä»»åŠ¡è¿”å› None
        """
        if not REDIS_QUEUE_AVAILABLE:
            return None

        redis_queue = get_redis_queue()
        if not redis_queue:
            return None

        try:
            # ä» Redis è·å–ä»»åŠ¡ IDï¼ˆé˜»å¡å¼ï¼Œ1ç§’è¶…æ—¶ï¼‰
            task_id = redis_queue.dequeue(worker_id, timeout=1.0)
            if not task_id:
                return None

            # ä» SQLite è·å–å®Œæ•´ä»»åŠ¡æ•°æ®
            with self.get_cursor() as cursor:
                cursor.execute("SELECT * FROM tasks WHERE task_id = ?", (task_id,))
                task = cursor.fetchone()

                if not task:
                    # ä»»åŠ¡åœ¨ Redis ä¸­ä½†ä¸åœ¨ SQLite ä¸­ï¼ˆå¼‚å¸¸æƒ…å†µï¼‰
                    logger.error(f"âŒ Task {task_id} found in Redis but not in SQLite")
                    redis_queue.fail(task_id, worker_id, requeue=False)
                    return None

                # æ›´æ–° SQLite ä¸­çš„ä»»åŠ¡çŠ¶æ€
                cursor.execute(
                    """
                    UPDATE tasks
                    SET status = 'processing',
                        started_at = CURRENT_TIMESTAMP,
                        worker_id = ?
                    WHERE task_id = ? AND status = 'pending'
                    """,
                    (worker_id, task_id),
                )

                if cursor.rowcount == 0:
                    # ä»»åŠ¡çŠ¶æ€å·²ç»æ”¹å˜ï¼ˆå¯èƒ½è¢«å–æ¶ˆç­‰ï¼‰
                    logger.warning(f"âš ï¸  Task {task_id} status changed, skipping")
                    redis_queue.fail(task_id, worker_id, requeue=False)
                    return None

                logger.info(f"ğŸ“¤ [Redis] Task {task_id} claimed by worker {worker_id}")
                return dict(task)

        except Exception as e:
            logger.error(f"âŒ Redis dequeue failed, falling back to SQLite: {e}")
            return None

    def update_task_status(
        self, task_id: str, status: str, result_path: str = None, error_message: str = None, worker_id: str = None
    ):
        """
        æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼ˆä½¿ç”¨é¢„å®šä¹‰ SQL æ¨¡æ¿ï¼Œé˜²æ­¢ SQL æ³¨å…¥ï¼‰

        Args:
            task_id: ä»»åŠ¡ID
            status: æ–°çŠ¶æ€ (pending/processing/completed/failed/cancelled)
            result_path: ç»“æœè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            error_message: é”™è¯¯ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            worker_id: Worker IDï¼ˆå¯é€‰ï¼Œç”¨äºå¹¶å‘æ£€æŸ¥ï¼‰

        Returns:
            bool: æ›´æ–°æ˜¯å¦æˆåŠŸ

        å¹¶å‘å®‰å…¨è¯´æ˜ï¼š
            1. æ›´æ–°ä¸º completed/failed æ—¶ä¼šæ£€æŸ¥çŠ¶æ€æ˜¯ processing
            2. å¦‚æœæä¾› worker_idï¼Œä¼šæ£€æŸ¥ä»»åŠ¡æ˜¯å¦å±äºè¯¥ worker
            3. è¿”å› False è¡¨ç¤ºä»»åŠ¡è¢«å…¶ä»–è¿›ç¨‹ä¿®æ”¹äº†

        å®‰å…¨è¯´æ˜ï¼š
            ä½¿ç”¨é¢„å®šä¹‰çš„ SQL æ¨¡æ¿ï¼Œå®Œå…¨é¿å… SQL æ³¨å…¥é£é™©
        """
        with self.get_cursor() as cursor:
            success = False

            # æ ¹æ®ä¸åŒçŠ¶æ€ä½¿ç”¨é¢„å®šä¹‰çš„ SQL æ¨¡æ¿
            if status == "completed":
                # å®ŒæˆçŠ¶æ€ï¼šæ›´æ–°çŠ¶æ€ã€å®Œæˆæ—¶é—´å’Œç»“æœè·¯å¾„
                if worker_id:
                    # å¸¦ worker_id éªŒè¯
                    sql = """
                        UPDATE tasks
                        SET status = ?,
                            completed_at = CURRENT_TIMESTAMP,
                            result_path = ?
                        WHERE task_id = ?
                        AND status = 'processing'
                        AND worker_id = ?
                    """
                    cursor.execute(sql, (status, result_path, task_id, worker_id))
                else:
                    # ä¸éªŒè¯ worker_id
                    sql = """
                        UPDATE tasks
                        SET status = ?,
                            completed_at = CURRENT_TIMESTAMP,
                            result_path = ?
                        WHERE task_id = ?
                        AND status = 'processing'
                    """
                    cursor.execute(sql, (status, result_path, task_id))

                success = cursor.rowcount > 0

            elif status == "failed":
                # å¤±è´¥çŠ¶æ€ï¼šæ›´æ–°çŠ¶æ€ã€å®Œæˆæ—¶é—´å’Œé”™è¯¯ä¿¡æ¯
                if worker_id:
                    # å¸¦ worker_id éªŒè¯
                    sql = """
                        UPDATE tasks
                        SET status = ?,
                            completed_at = CURRENT_TIMESTAMP,
                            error_message = ?
                        WHERE task_id = ?
                        AND status = 'processing'
                        AND worker_id = ?
                    """
                    cursor.execute(sql, (status, error_message, task_id, worker_id))
                else:
                    # ä¸éªŒè¯ worker_id
                    sql = """
                        UPDATE tasks
                        SET status = ?,
                            completed_at = CURRENT_TIMESTAMP,
                            error_message = ?
                        WHERE task_id = ?
                        AND status = 'processing'
                    """
                    cursor.execute(sql, (status, error_message, task_id))

                success = cursor.rowcount > 0

            elif status == "cancelled":
                # å–æ¶ˆçŠ¶æ€ï¼šç›´æ¥æ›´æ–°çŠ¶æ€
                sql = """
                    UPDATE tasks
                    SET status = ?,
                        completed_at = CURRENT_TIMESTAMP
                    WHERE task_id = ?
                """
                cursor.execute(sql, (status, task_id))
                success = cursor.rowcount > 0

            elif status == "pending":
                # é‡ç½®ä¸ºå¾…å¤„ç†çŠ¶æ€
                sql = """
                    UPDATE tasks
                    SET status = ?,
                        worker_id = NULL,
                        started_at = NULL
                    WHERE task_id = ?
                """
                cursor.execute(sql, (status, task_id))
                success = cursor.rowcount > 0

            else:
                # å…¶ä»–çŠ¶æ€ï¼ˆå¦‚ processingï¼‰ï¼šç®€å•æ›´æ–°çŠ¶æ€
                sql = """
                    UPDATE tasks
                    SET status = ?
                    WHERE task_id = ?
                """
                cursor.execute(sql, (status, task_id))
                success = cursor.rowcount > 0

            # è°ƒè¯•æ—¥å¿—ï¼ˆä»…åœ¨å¤±è´¥æ—¶ï¼‰
            if not success and status in ["completed", "failed"]:
                from loguru import logger

                logger.debug(f"Status update failed: task_id={task_id}, status={status}, " f"worker_id={worker_id}")

            # é€šçŸ¥ Redis ä»»åŠ¡å®Œæˆ/å¤±è´¥ï¼ˆæ¸…ç† processing setï¼‰
            if success and status in ["completed", "failed"]:
                self._notify_redis_task_done(task_id, worker_id or "", status)

            return success

    def _notify_redis_task_done(self, task_id: str, worker_id: str, status: str):
        """
        é€šçŸ¥ Redis ä»»åŠ¡å·²å®Œæˆ/å¤±è´¥

        ä» processing set ä¸­ç§»é™¤ä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID
            worker_id: Worker ID
            status: æœ€ç»ˆçŠ¶æ€ (completed/failed)
        """
        if not REDIS_QUEUE_AVAILABLE:
            return

        redis_queue = get_redis_queue()
        if redis_queue:
            try:
                if status == "completed":
                    redis_queue.complete(task_id, worker_id)
                else:
                    redis_queue.fail(task_id, worker_id, requeue=False)
            except Exception as e:
                # Redis æ¸…ç†å¤±è´¥ä¸å½±å“ä»»åŠ¡å®Œæˆ
                logger.warning(f"âš ï¸  Failed to notify Redis about task {task_id}: {e}")

    def get_task(self, task_id: str) -> Optional[Dict]:
        """
        æŸ¥è¯¢ä»»åŠ¡è¯¦æƒ…

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            task: ä»»åŠ¡å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        with self.get_cursor() as cursor:
            cursor.execute("SELECT * FROM tasks WHERE task_id = ?", (task_id,))
            task = cursor.fetchone()
            return dict(task) if task else None

    def get_queue_stats(self) -> Dict[str, int]:
        """
        è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯

        Returns:
            stats: å„çŠ¶æ€çš„ä»»åŠ¡æ•°é‡ï¼ŒåŒ…å« Redis é˜Ÿåˆ—ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        """
        with self.get_cursor() as cursor:
            cursor.execute("""
                SELECT status, COUNT(*) as count
                FROM tasks
                GROUP BY status
            """)
            stats = {row["status"]: row["count"] for row in cursor.fetchall()}

        # æ·»åŠ  Redis é˜Ÿåˆ—ç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if REDIS_QUEUE_AVAILABLE:
            redis_queue = get_redis_queue()
            if redis_queue:
                try:
                    redis_stats = redis_queue.get_stats()
                    stats["_redis_enabled"] = True
                    stats["_redis_pending"] = redis_stats.get("pending", 0)
                    stats["_redis_processing"] = redis_stats.get("processing", 0)
                except Exception as e:
                    stats["_redis_enabled"] = False
                    stats["_redis_error"] = str(e)
            else:
                stats["_redis_enabled"] = False
        else:
            stats["_redis_enabled"] = False

        return stats

    def get_tasks_by_status(self, status: str, limit: int = 100) -> List[Dict]:
        """
        æ ¹æ®çŠ¶æ€è·å–ä»»åŠ¡åˆ—è¡¨

        Args:
            status: ä»»åŠ¡çŠ¶æ€
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            tasks: ä»»åŠ¡åˆ—è¡¨
        """
        with self.get_cursor() as cursor:
            cursor.execute(
                """
                SELECT * FROM tasks
                WHERE status = ?
                ORDER BY created_at DESC
                LIMIT ?
            """,
                (status, limit),
            )
            return [dict(row) for row in cursor.fetchall()]

    def cleanup_old_task_records(self, days: int = 30):
        """
        æ¸…ç†æ—§ä»»åŠ¡ï¼ˆåŒæ—¶åˆ é™¤æ‰€æœ‰ç›¸å…³æ–‡ä»¶å’Œæ•°æ®åº“è®°å½•ï¼‰

        Args:
            days: åˆ é™¤å¤šå°‘å¤©å‰çš„ä»»åŠ¡

        Returns:
            int: åˆ é™¤çš„ä»»åŠ¡è®°å½•æ•°

        æ¸…ç†å†…å®¹ï¼š
            - ä¸Šä¼ çš„åŸå§‹æ–‡ä»¶ï¼ˆfile_pathï¼‰
            - ç»“æœæ–‡ä»¶å¤¹åŠå…¶æ‰€æœ‰å†…å®¹ï¼ˆresult_pathï¼ŒåŒ…æ‹¬ç”Ÿæˆçš„æ–‡ä»¶å’Œä¸­é—´æ–‡ä»¶ï¼‰
            - æ•°æ®åº“è®°å½•

        æ³¨æ„ï¼š
            - æ“ä½œä¸å¯æ¢å¤
            - å»ºè®®è®¾ç½®åˆç†çš„ä¿ç•™æœŸï¼ˆå¦‚ 7-30 å¤©ï¼‰
            - ç”±å®šæ—¶ä»»åŠ¡è‡ªåŠ¨æ‰§è¡Œï¼Œä¹Ÿå¯é€šè¿‡ç®¡ç†æ¥å£æ‰‹åŠ¨è§¦å‘
        """
        from pathlib import Path
        import shutil

        with self.get_cursor() as cursor:
            # å…ˆæŸ¥è¯¢è¦åˆ é™¤çš„ä»»åŠ¡åŠå…¶æ–‡ä»¶è·¯å¾„ï¼ˆåŒ…æ‹¬ä¸Šä¼ æ–‡ä»¶å’Œç»“æœæ–‡ä»¶ï¼‰
            cursor.execute(
                """
                SELECT task_id, file_path, result_path FROM tasks
                WHERE completed_at < datetime('now', '-' || ? || ' days')
                AND status IN ('completed', 'failed')
            """,
                (days,),
            )

            old_tasks = cursor.fetchall()

            # åˆ é™¤æ‰€æœ‰ç›¸å…³æ–‡ä»¶
            for task in old_tasks:
                task_id = task["task_id"]

                # 1. åˆ é™¤ä¸Šä¼ çš„åŸå§‹æ–‡ä»¶
                if task["file_path"]:
                    file_path = Path(task["file_path"])
                    if file_path.exists() and file_path.is_file():
                        try:
                            file_path.unlink()
                            logger.debug(f"Deleted upload file for task {task_id}: {file_path.name}")
                        except Exception as e:
                            logger.warning(f"Failed to delete upload file for task {task_id}: {e}")

                # 2. åˆ é™¤ç»“æœæ–‡ä»¶å¤¹ï¼ˆåŒ…å«æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶å’Œä¸­é—´æ–‡ä»¶ï¼‰
                if task["result_path"]:
                    result_path = Path(task["result_path"])
                    if result_path.exists() and result_path.is_dir():
                        try:
                            shutil.rmtree(result_path)
                            logger.debug(f"Deleted result directory for task {task_id}: {result_path.name}")
                        except Exception as e:
                            logger.warning(f"Failed to delete result files for task {task_id}: {e}")

            # åˆ é™¤æ•°æ®åº“è®°å½•
            cursor.execute(
                """
                DELETE FROM tasks
                WHERE completed_at < datetime('now', '-' || ? || ' days')
                AND status IN ('completed', 'failed')
            """,
                (days,),
            )

            deleted_count = cursor.rowcount
            return deleted_count

    def reset_stale_tasks(self, timeout_minutes: int = 60):
        """
        é‡ç½®è¶…æ—¶çš„ processing ä»»åŠ¡ä¸º pending

        Args:
            timeout_minutes: è¶…æ—¶æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
        """
        with self.get_cursor() as cursor:
            cursor.execute(
                """
                UPDATE tasks
                SET status = 'pending',
                    worker_id = NULL,
                    retry_count = retry_count + 1
                WHERE status = 'processing'
                AND started_at < datetime('now', '-' || ? || ' minutes')
            """,
                (timeout_minutes,),
            )
            reset_count = cursor.rowcount
            return reset_count

    # ============================================================================
    # ä¸»å­ä»»åŠ¡æ”¯æŒ (Parent-Child Task Support)
    # ============================================================================

    def create_parent_task(
        self,
        file_name: str,
        file_path: str,
        backend: str = "pipeline",
        options: dict = None,
        priority: int = 0,
        user_id: str = None,
    ) -> str:
        """
        åˆ›å»ºä¸»ä»»åŠ¡ï¼ˆç”¨äºå¤§æ–‡ä»¶æ‹†åˆ†ï¼‰

        Args:
            file_name: åŸå§‹æ–‡ä»¶å
            file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
            backend: å¤„ç†åç«¯
            options: å¤„ç†é€‰é¡¹
            priority: ä¼˜å…ˆçº§
            user_id: ç”¨æˆ·ID

        Returns:
            task_id: ä¸»ä»»åŠ¡ID
        """
        task_id = str(uuid.uuid4())
        with self.get_cursor() as cursor:
            cursor.execute(
                """
                INSERT INTO tasks (
                    task_id, file_name, file_path, backend, options,
                    status, priority, user_id, is_parent, child_count
                ) VALUES (?, ?, ?, ?, ?, 'processing', ?, ?, 1, 0)
            """,
                (task_id, file_name, file_path, backend, json.dumps(options or {}), priority, user_id),
            )
        logger.info(f"ğŸ“‹ Created parent task: {task_id}")
        return task_id

    def convert_to_parent_task(self, task_id: str, child_count: int = 0):
        """
        å°†æ™®é€šä»»åŠ¡è½¬æ¢ä¸ºçˆ¶ä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID
            child_count: å­ä»»åŠ¡æ•°é‡
        """
        with self.get_cursor() as cursor:
            cursor.execute(
                """
                UPDATE tasks
                SET is_parent = 1, child_count = ?, status = 'processing'
                WHERE task_id = ?
                """,
                (child_count, task_id),
            )
        logger.info(f"ğŸ”„ Converted task {task_id} to parent task with {child_count} children")

    def create_child_task(
        self,
        parent_task_id: str,
        file_name: str,
        file_path: str,
        backend: str = "pipeline",
        options: dict = None,
        priority: int = 0,
        user_id: str = None,
    ) -> str:
        """
        åˆ›å»ºå­ä»»åŠ¡

        Args:
            parent_task_id: çˆ¶ä»»åŠ¡ID
            file_name: åˆ†ç‰‡æ–‡ä»¶å
            file_path: åˆ†ç‰‡æ–‡ä»¶è·¯å¾„
            backend: å¤„ç†åç«¯
            options: å¤„ç†é€‰é¡¹ï¼ˆåŒ…å« chunk_infoï¼‰
            priority: ä¼˜å…ˆçº§ï¼ˆç»§æ‰¿çˆ¶ä»»åŠ¡ï¼‰
            user_id: ç”¨æˆ·IDï¼ˆç»§æ‰¿çˆ¶ä»»åŠ¡ï¼‰

        Returns:
            task_id: å­ä»»åŠ¡ID
        """
        task_id = str(uuid.uuid4())
        with self.get_cursor() as cursor:
            # åˆ›å»ºå­ä»»åŠ¡
            cursor.execute(
                """
                INSERT INTO tasks (
                    task_id, parent_task_id, file_name, file_path,
                    backend, options, status, priority, user_id
                ) VALUES (?, ?, ?, ?, ?, ?, 'pending', ?, ?)
            """,
                (
                    task_id,
                    parent_task_id,
                    file_name,
                    file_path,
                    backend,
                    json.dumps(options or {}),
                    priority,
                    user_id,
                ),
            )

            # æ›´æ–°çˆ¶ä»»åŠ¡çš„å­ä»»åŠ¡è®¡æ•°
            cursor.execute(
                """
                UPDATE tasks
                SET child_count = child_count + 1
                WHERE task_id = ?
            """,
                (parent_task_id,),
            )

        logger.debug(f"ğŸ“„ Created child task: {task_id} (parent: {parent_task_id})")
        return task_id

    def on_child_task_completed(self, child_task_id: str) -> Optional[str]:
        """
        å­ä»»åŠ¡å®Œæˆå›è°ƒ

        å½“å­ä»»åŠ¡å®Œæˆæ—¶è°ƒç”¨ï¼Œæ›´æ–°çˆ¶ä»»åŠ¡çš„å®Œæˆè®¡æ•°
        å¦‚æœæ‰€æœ‰å­ä»»åŠ¡éƒ½å®Œæˆï¼Œè¿”å›çˆ¶ä»»åŠ¡IDç”¨äºè§¦å‘åˆå¹¶

        Args:
            child_task_id: å­ä»»åŠ¡ID

        Returns:
            parent_task_id: å¦‚æœæ‰€æœ‰å­ä»»åŠ¡å®Œæˆï¼Œè¿”å›çˆ¶ä»»åŠ¡IDï¼›å¦åˆ™è¿”å› None
        """
        with self.get_cursor() as cursor:
            # è·å–çˆ¶ä»»åŠ¡ID
            cursor.execute(
                """
                SELECT parent_task_id FROM tasks WHERE task_id = ?
            """,
                (child_task_id,),
            )
            row = cursor.fetchone()

            if not row or not row["parent_task_id"]:
                return None  # ä¸æ˜¯å­ä»»åŠ¡

            parent_task_id = row["parent_task_id"]

            # æ›´æ–°çˆ¶ä»»åŠ¡çš„å®Œæˆè®¡æ•°
            cursor.execute(
                """
                UPDATE tasks
                SET child_completed = child_completed + 1
                WHERE task_id = ?
            """,
                (parent_task_id,),
            )

            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å­ä»»åŠ¡éƒ½å®Œæˆäº†
            cursor.execute(
                """
                SELECT child_count, child_completed, file_name
                FROM tasks WHERE task_id = ?
            """,
                (parent_task_id,),
            )
            parent = cursor.fetchone()

            if parent and parent["child_completed"] >= parent["child_count"]:
                # æ‰€æœ‰å­ä»»åŠ¡å®Œæˆ
                logger.info(
                    f"ğŸ‰ All subtasks completed for parent task {parent_task_id} "
                    f"({parent['child_completed']}/{parent['child_count']}) - {parent['file_name']}"
                )
                return parent_task_id

            if parent:
                logger.info(
                    f"â³ Subtask progress: {parent['child_completed']}/{parent['child_count']} "
                    f"for parent task {parent_task_id}"
                )

        return None

    def on_child_task_failed(self, child_task_id: str, error_message: str):
        """
        å­ä»»åŠ¡å¤±è´¥å›è°ƒ

        å½“å­ä»»åŠ¡å¤±è´¥æ—¶ï¼Œæ ‡è®°çˆ¶ä»»åŠ¡ä¸ºå¤±è´¥çŠ¶æ€

        Args:
            child_task_id: å­ä»»åŠ¡ID
            error_message: é”™è¯¯ä¿¡æ¯
        """
        with self.get_cursor() as cursor:
            # è·å–çˆ¶ä»»åŠ¡ID
            cursor.execute(
                """
                SELECT parent_task_id FROM tasks WHERE task_id = ?
            """,
                (child_task_id,),
            )
            row = cursor.fetchone()

            if not row or not row["parent_task_id"]:
                return  # ä¸æ˜¯å­ä»»åŠ¡

            parent_task_id = row["parent_task_id"]

            # æ ‡è®°çˆ¶ä»»åŠ¡ä¸ºå¤±è´¥
            cursor.execute(
                """
                UPDATE tasks
                SET status = 'failed',
                    completed_at = CURRENT_TIMESTAMP,
                    error_message = ?
                WHERE task_id = ?
                AND status = 'processing'
            """,
                (f"Subtask {child_task_id} failed: {error_message}", parent_task_id),
            )

            if cursor.rowcount > 0:
                logger.error(f"âŒ Parent task {parent_task_id} marked as failed due to subtask failure")

    def get_task_with_children(self, task_id: str) -> Optional[Dict]:
        """
        è·å–ä»»åŠ¡åŠå…¶æ‰€æœ‰å­ä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            task: åŒ…å« children å­—æ®µçš„ä»»åŠ¡å­—å…¸
        """
        with self.get_cursor() as cursor:
            # è·å–ä¸»ä»»åŠ¡
            cursor.execute("SELECT * FROM tasks WHERE task_id = ?", (task_id,))
            parent_row = cursor.fetchone()

            if not parent_row:
                return None

            parent = dict(parent_row)

            # å¦‚æœæ˜¯ä¸»ä»»åŠ¡ï¼Œè·å–æ‰€æœ‰å­ä»»åŠ¡
            if parent.get("is_parent"):
                cursor.execute(
                    """
                    SELECT * FROM tasks
                    WHERE parent_task_id = ?
                    ORDER BY created_at
                """,
                    (task_id,),
                )
                children = [dict(row) for row in cursor.fetchall()]
                parent["children"] = children
            else:
                parent["children"] = []

            return parent

    def get_child_tasks(self, parent_task_id: str) -> List[Dict]:
        """
        è·å–çˆ¶ä»»åŠ¡çš„æ‰€æœ‰å­ä»»åŠ¡

        Args:
            parent_task_id: çˆ¶ä»»åŠ¡ID

        Returns:
            children: å­ä»»åŠ¡åˆ—è¡¨
        """
        with self.get_cursor() as cursor:
            cursor.execute(
                """
                SELECT * FROM tasks
                WHERE parent_task_id = ?
                ORDER BY created_at
            """,
                (parent_task_id,),
            )
            return [dict(row) for row in cursor.fetchall()]


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    db = TaskDB("test_tianshu.db")

    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    task_id = db.create_task(
        file_name="test.pdf",
        file_path="/tmp/test.pdf",
        backend="pipeline",
        options={"lang": "ch", "formula_enable": True},
        priority=1,
    )
    print(f"Created task: {task_id}")

    # æŸ¥è¯¢ä»»åŠ¡
    task = db.get_task(task_id)
    print(f"Task details: {task}")

    # è·å–ç»Ÿè®¡
    stats = db.get_queue_stats()
    print(f"Queue stats: {stats}")

    # æ¸…ç†æµ‹è¯•æ•°æ®åº“
    Path("test_tianshu.db").unlink(missing_ok=True)
    print("Test completed!")
