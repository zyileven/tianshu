"""
MinerU Tianshu - Redis Task Queue
å¤©æ¢ Redis ä»»åŠ¡é˜Ÿåˆ—

é«˜æ€§èƒ½ä»»åŠ¡é˜Ÿåˆ—å®ç°ï¼Œç”¨äºè§£å†³ SQLite å•å†™é”ç“¶é¢ˆ
æ”¯æŒä¼˜å…ˆçº§é˜Ÿåˆ—ã€å¯é æŠ•é€’ã€æ°´å¹³æ‰©å±•

æ¶æ„:
    - Redis Sorted Set å®ç°ä¼˜å…ˆçº§é˜Ÿåˆ—
    - Processing Set è·Ÿè¸ªè¿›è¡Œä¸­çš„ä»»åŠ¡
    - SQLite ä»ä¿ç•™ä»»åŠ¡å…ƒæ•°æ®å­˜å‚¨ï¼ˆå†å²è®°å½•ã€ç»“æœï¼‰
"""

import os
import time
import json
from typing import Optional, Dict, List
from dataclasses import dataclass
from loguru import logger

try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    logger.warning("âš ï¸  redis package not installed. Run: pip install redis")


@dataclass
class RedisConfig:
    """Redis é…ç½®"""
    host: str = "localhost"
    port: int = 6379
    db: int = 0
    password: Optional[str] = None
    socket_timeout: float = 5.0
    socket_connect_timeout: float = 5.0
    retry_on_timeout: bool = True

    # é˜Ÿåˆ—é…ç½®
    queue_key: str = "tianshu:task_queue"          # ä¼˜å…ˆçº§é˜Ÿåˆ— (Sorted Set)
    processing_key: str = "tianshu:processing"     # å¤„ç†ä¸­ä»»åŠ¡ (Set)
    task_data_prefix: str = "tianshu:task:"        # ä»»åŠ¡æ•°æ®å‰ç¼€ (Hash)

    # è¶…æ—¶é…ç½®
    task_timeout_seconds: int = 3600               # ä»»åŠ¡è¶…æ—¶æ—¶é—´ (1å°æ—¶)
    claim_visibility_seconds: int = 300            # ä»»åŠ¡å¯è§æ€§è¶…æ—¶ (5åˆ†é’Ÿ)

    @classmethod
    def from_env(cls) -> "RedisConfig":
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        return cls(
            host=os.getenv("REDIS_HOST", "localhost"),
            port=int(os.getenv("REDIS_PORT", "6379")),
            db=int(os.getenv("REDIS_DB", "0")),
            password=os.getenv("REDIS_PASSWORD") or None,
            queue_key=os.getenv("REDIS_QUEUE_KEY", "tianshu:task_queue"),
            processing_key=os.getenv("REDIS_PROCESSING_KEY", "tianshu:processing"),
            task_timeout_seconds=int(os.getenv("REDIS_TASK_TIMEOUT", "3600")),
        )


class RedisTaskQueue:
    """
    Redis ä»»åŠ¡é˜Ÿåˆ—

    ä½¿ç”¨ Sorted Set å®ç°ä¼˜å…ˆçº§é˜Ÿåˆ—:
        - score = -priority * 1e10 + timestamp (ä¼˜å…ˆçº§é«˜çš„å…ˆå‡ºï¼ŒåŒä¼˜å…ˆçº§æŒ‰æ—¶é—´)
        - BZPOPMIN é˜»å¡è·å–æœ€é«˜ä¼˜å…ˆçº§ä»»åŠ¡

    å¯é æŠ•é€’:
        - ä»»åŠ¡ä» queue ç§»åŠ¨åˆ° processing set
        - Worker å¿ƒè·³æ›´æ–°å¤„ç†æ—¶é—´
        - è¶…æ—¶ä»»åŠ¡è‡ªåŠ¨é‡æ–°å…¥é˜Ÿ
    """

    def __init__(self, config: Optional[RedisConfig] = None):
        """
        åˆå§‹åŒ– Redis é˜Ÿåˆ—

        Args:
            config: Redis é…ç½®ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡åŠ è½½
        """
        if not REDIS_AVAILABLE:
            raise RuntimeError("redis package not installed. Run: pip install redis")

        self.config = config or RedisConfig.from_env()
        self._client: Optional[redis.Redis] = None
        self._connected = False

    @property
    def client(self) -> redis.Redis:
        """è·å– Redis å®¢æˆ·ç«¯ï¼ˆå»¶è¿Ÿè¿æ¥ï¼‰"""
        if self._client is None:
            self._client = redis.Redis(
                host=self.config.host,
                port=self.config.port,
                db=self.config.db,
                password=self.config.password,
                socket_timeout=self.config.socket_timeout,
                socket_connect_timeout=self.config.socket_connect_timeout,
                retry_on_timeout=self.config.retry_on_timeout,
                decode_responses=True,
            )
        return self._client

    def is_available(self) -> bool:
        """æ£€æŸ¥ Redis æ˜¯å¦å¯ç”¨"""
        try:
            self.client.ping()
            self._connected = True
            return True
        except Exception as e:
            logger.warning(f"Redis not available: {e}")
            self._connected = False
            return False

    def enqueue(
        self,
        task_id: str,
        priority: int = 0,
        task_data: Optional[Dict] = None,
    ) -> bool:
        """
        å°†ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—

        Args:
            task_id: ä»»åŠ¡ID
            priority: ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå¤§è¶Šä¼˜å…ˆï¼‰
            task_data: ä»»åŠ¡æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºå¿«é€Ÿè®¿é—®ï¼‰

        Returns:
            bool: æ˜¯å¦æˆåŠŸå…¥é˜Ÿ
        """
        try:
            # è®¡ç®—åˆ†æ•°ï¼šä¼˜å…ˆçº§é«˜çš„å…ˆå‡ºï¼ŒåŒä¼˜å…ˆçº§æŒ‰æ—¶é—´å…ˆå
            # score = -priority * 1e10 + timestamp
            timestamp = time.time()
            score = -priority * 1e10 + timestamp

            pipe = self.client.pipeline()

            # æ·»åŠ åˆ°ä¼˜å…ˆçº§é˜Ÿåˆ—
            pipe.zadd(self.config.queue_key, {task_id: score})

            # å­˜å‚¨ä»»åŠ¡æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºå¿«é€Ÿè®¿é—®ï¼‰
            if task_data:
                task_key = f"{self.config.task_data_prefix}{task_id}"
                pipe.hset(task_key, mapping={
                    "task_id": task_id,
                    "priority": str(priority),
                    "enqueued_at": str(timestamp),
                    "data": json.dumps(task_data),
                })
                # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆä»»åŠ¡è¶…æ—¶åè‡ªåŠ¨æ¸…ç†ï¼‰
                pipe.expire(task_key, self.config.task_timeout_seconds)

            pipe.execute()
            logger.debug(f"ğŸ“¥ Task {task_id} enqueued with priority {priority}")
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to enqueue task {task_id}: {e}")
            return False

    def dequeue(
        self,
        worker_id: str,
        timeout: float = 1.0,
    ) -> Optional[str]:
        """
        ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ï¼ˆé˜»å¡å¼ï¼‰

        ä½¿ç”¨ BZPOPMIN åŸå­æ“ä½œè·å–æœ€é«˜ä¼˜å…ˆçº§ä»»åŠ¡
        ä»»åŠ¡ä¼šç§»å…¥ processing setï¼Œé˜²æ­¢é‡å¤å¤„ç†

        Args:
            worker_id: Worker ID
            timeout: é˜»å¡è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            task_id: ä»»åŠ¡IDï¼Œå¦‚æœæ²¡æœ‰ä»»åŠ¡è¿”å› None
        """
        try:
            # ä½¿ç”¨ BZPOPMIN é˜»å¡è·å–æœ€å° score çš„å…ƒç´ ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            result = self.client.bzpopmin(self.config.queue_key, timeout=timeout)

            if result is None:
                return None

            # result = (key, member, score)
            _, task_id, _ = result

            # å°†ä»»åŠ¡æ·»åŠ åˆ° processing setï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
            processing_data = json.dumps({
                "worker_id": worker_id,
                "claimed_at": time.time(),
            })
            self.client.hset(self.config.processing_key, task_id, processing_data)

            logger.debug(f"ğŸ“¤ Task {task_id} claimed by worker {worker_id}")
            return task_id

        except Exception as e:
            logger.error(f"âŒ Failed to dequeue task: {e}")
            return None

    def complete(self, task_id: str, worker_id: str) -> bool:
        """
        æ ‡è®°ä»»åŠ¡å®Œæˆ

        ä» processing set ä¸­ç§»é™¤ä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID
            worker_id: Worker ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            # ä» processing set ç§»é™¤
            self.client.hdel(self.config.processing_key, task_id)

            # åˆ é™¤ä»»åŠ¡æ•°æ®ç¼“å­˜
            task_key = f"{self.config.task_data_prefix}{task_id}"
            self.client.delete(task_key)

            logger.debug(f"âœ… Task {task_id} completed by worker {worker_id}")
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to complete task {task_id}: {e}")
            return False

    def fail(self, task_id: str, worker_id: str, requeue: bool = False) -> bool:
        """
        æ ‡è®°ä»»åŠ¡å¤±è´¥

        Args:
            task_id: ä»»åŠ¡ID
            worker_id: Worker ID
            requeue: æ˜¯å¦é‡æ–°å…¥é˜Ÿ

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            # ä» processing set ç§»é™¤
            self.client.hdel(self.config.processing_key, task_id)

            if requeue:
                # é‡æ–°å…¥é˜Ÿï¼ˆä¿æŒåŸä¼˜å…ˆçº§ï¼‰
                task_key = f"{self.config.task_data_prefix}{task_id}"
                task_info = self.client.hgetall(task_key)
                priority = int(task_info.get("priority", "0"))

                timestamp = time.time()
                score = -priority * 1e10 + timestamp
                self.client.zadd(self.config.queue_key, {task_id: score})
                logger.info(f"ğŸ”„ Task {task_id} requeued after failure")
            else:
                # åˆ é™¤ä»»åŠ¡æ•°æ®ç¼“å­˜
                task_key = f"{self.config.task_data_prefix}{task_id}"
                self.client.delete(task_key)
                logger.debug(f"âŒ Task {task_id} failed (not requeued)")

            return True

        except Exception as e:
            logger.error(f"âŒ Failed to mark task {task_id} as failed: {e}")
            return False

    def heartbeat(self, task_id: str, worker_id: str) -> bool:
        """
        æ›´æ–°ä»»åŠ¡å¿ƒè·³

        é˜²æ­¢ä»»åŠ¡å› è¶…æ—¶è¢«é‡æ–°åˆ†é…

        Args:
            task_id: ä»»åŠ¡ID
            worker_id: Worker ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            processing_data = json.dumps({
                "worker_id": worker_id,
                "claimed_at": time.time(),  # æ›´æ–°æ—¶é—´
            })
            self.client.hset(self.config.processing_key, task_id, processing_data)
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to update heartbeat for task {task_id}: {e}")
            return False

    def recover_stale_tasks(self, timeout_seconds: Optional[int] = None) -> int:
        """
        æ¢å¤è¶…æ—¶ä»»åŠ¡

        æ£€æŸ¥ processing set ä¸­è¶…æ—¶çš„ä»»åŠ¡ï¼Œé‡æ–°å…¥é˜Ÿ

        Args:
            timeout_seconds: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼

        Returns:
            int: æ¢å¤çš„ä»»åŠ¡æ•°é‡
        """
        timeout = timeout_seconds or self.config.claim_visibility_seconds
        recovered_count = 0
        now = time.time()

        try:
            # è·å–æ‰€æœ‰å¤„ç†ä¸­çš„ä»»åŠ¡
            processing_tasks = self.client.hgetall(self.config.processing_key)

            for task_id, data_str in processing_tasks.items():
                try:
                    data = json.loads(data_str)
                    claimed_at = data.get("claimed_at", 0)

                    if now - claimed_at > timeout:
                        # ä»»åŠ¡è¶…æ—¶ï¼Œé‡æ–°å…¥é˜Ÿ
                        worker_id = data.get("worker_id", "unknown")
                        logger.warning(
                            f"âš ï¸  Task {task_id} timed out (worker: {worker_id}), requeuing..."
                        )

                        # ä» processing ç§»é™¤
                        self.client.hdel(self.config.processing_key, task_id)

                        # é‡æ–°å…¥é˜Ÿï¼ˆé»˜è®¤ä¼˜å…ˆçº§ï¼‰
                        task_key = f"{self.config.task_data_prefix}{task_id}"
                        task_info = self.client.hgetall(task_key)
                        priority = int(task_info.get("priority", "0"))

                        score = -priority * 1e10 + now
                        self.client.zadd(self.config.queue_key, {task_id: score})
                        recovered_count += 1

                except json.JSONDecodeError:
                    logger.error(f"Invalid processing data for task {task_id}")

            if recovered_count > 0:
                logger.info(f"ğŸ”„ Recovered {recovered_count} stale tasks")

            return recovered_count

        except Exception as e:
            logger.error(f"âŒ Failed to recover stale tasks: {e}")
            return 0

    def get_stats(self) -> Dict:
        """
        è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯

        Returns:
            dict: é˜Ÿåˆ—ç»Ÿè®¡
        """
        try:
            pending_count = self.client.zcard(self.config.queue_key)
            processing_count = self.client.hlen(self.config.processing_key)

            return {
                "pending": pending_count,
                "processing": processing_count,
                "redis_connected": True,
            }
        except Exception as e:
            logger.error(f"âŒ Failed to get queue stats: {e}")
            return {
                "pending": 0,
                "processing": 0,
                "redis_connected": False,
                "error": str(e),
            }

    def clear_queue(self) -> bool:
        """
        æ¸…ç©ºé˜Ÿåˆ—ï¼ˆå±é™©æ“ä½œï¼Œä»…ç”¨äºæµ‹è¯•/é‡ç½®ï¼‰

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            pipe = self.client.pipeline()
            pipe.delete(self.config.queue_key)
            pipe.delete(self.config.processing_key)
            # æ¸…ç†æ‰€æœ‰ä»»åŠ¡æ•°æ®
            keys = self.client.keys(f"{self.config.task_data_prefix}*")
            if keys:
                pipe.delete(*keys)
            pipe.execute()
            logger.warning("âš ï¸  Queue cleared!")
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to clear queue: {e}")
            return False


# å…¨å±€é˜Ÿåˆ—å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_queue_instance: Optional[RedisTaskQueue] = None


def get_redis_queue() -> Optional[RedisTaskQueue]:
    """
    è·å– Redis é˜Ÿåˆ—å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

    å¦‚æœ Redis ä¸å¯ç”¨ï¼Œè¿”å› Noneï¼ˆfallback åˆ° SQLiteï¼‰

    Returns:
        RedisTaskQueue æˆ– None
    """
    global _queue_instance

    if not REDIS_AVAILABLE:
        return None

    if _queue_instance is None:
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨ Redis é˜Ÿåˆ—
        if os.getenv("REDIS_QUEUE_ENABLED", "false").lower() != "true":
            logger.info("â„¹ï¸  Redis queue disabled (REDIS_QUEUE_ENABLED != true)")
            return None

        try:
            _queue_instance = RedisTaskQueue()
            if _queue_instance.is_available():
                logger.info(
                    f"âœ… Redis queue connected: "
                    f"{_queue_instance.config.host}:{_queue_instance.config.port}"
                )
            else:
                logger.warning("âš ï¸  Redis queue not available, falling back to SQLite")
                _queue_instance = None
        except Exception as e:
            logger.error(f"âŒ Failed to initialize Redis queue: {e}")
            _queue_instance = None

    return _queue_instance


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import os
    os.environ["REDIS_QUEUE_ENABLED"] = "true"

    queue = get_redis_queue()
    if queue and queue.is_available():
        print("âœ… Redis connected!")

        # æµ‹è¯•å…¥é˜Ÿ
        queue.enqueue("test-task-1", priority=10)
        queue.enqueue("test-task-2", priority=5)
        queue.enqueue("test-task-3", priority=10)  # åŒä¼˜å…ˆçº§ï¼Œåå…¥é˜Ÿ

        # æµ‹è¯•å‡ºé˜Ÿï¼ˆåº”è¯¥æŒ‰ä¼˜å…ˆçº§é¡ºåºï¼‰
        task1 = queue.dequeue("worker-1", timeout=1)
        print(f"Dequeued: {task1}")  # åº”è¯¥æ˜¯ test-task-1

        task2 = queue.dequeue("worker-1", timeout=1)
        print(f"Dequeued: {task2}")  # åº”è¯¥æ˜¯ test-task-3

        # è·å–ç»Ÿè®¡
        stats = queue.get_stats()
        print(f"Stats: {stats}")

        # æ¸…ç†
        queue.clear_queue()
        print("âœ… Test completed!")
    else:
        print("âš ï¸  Redis not available")
